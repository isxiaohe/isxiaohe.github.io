---
title: "NeRF: 用神经网络构建辐射场"
description: 介绍了经典文章 NeRF
date: 2025-10-27
tldr: NeRF 使用神经网络建模辐射场, 通过最小化体渲染结果和参考图片的误差来完成新视角合成和重建任务.
tags: ["neural rendering"]
---
这有一个 [Notion 版本](https://www.notion.so/NeRF-294a03b6573980eea642d35404ab7569?source=copy_link)

## Overview

方法首次提出于论文 [NeRF](https://arxiv.org/abs/2003.08934) . 其目的是为了解决经典的3D视觉任务多视角生成和重建. 该任务可以简单描述为 给定某个场景的一系列照片(我们假设已经进行了标定), 如何从中推理出这个场景任意视角的图像, 进一步的, 如何得到该场景的三维模型? NeRF 给出的解决方法是用神经网络去拟合场景的辐射场, 通过体渲染得到场景任意视角的图像.

整个过程: 选择光路→均匀采样 → 体渲染得到各采样点重要性 → 基于重要性采样 → 体渲染得到颜色

![image.png](/images/NeRF/image.png)

## 基于 Radiance Fields 的 Volume Rendering

NeRF 方法基于体渲染模型. 当我们想要渲染出一张图片时, 我们为每个像素确定一条光路, 该像素颜色 $C$ 通过在光线路径 $\mathbf{r}(t)= \mathbf{o} + t  \mathbf{d}$ 上积分得到

$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t), \mathbf{d})c(\mathbf{r}(t), \mathbf{d}) dt
$$

其中 $t_n$ $t_f$ 是起点与终点; $\sigma$ 是体密度函数, 从直观上来说, 表示光路上每个点的不透明度; $c$ 是颜色函数, 表示光路上每个点的颜色; $T$ 是光路上某个点的透明度, 用于表征光从起点传播到光路上某个点时产生的损失(因为路径上每个点都有一定的不透明度), 具体公式为

$$
T(t) = 1 - exp(-\int_{t_n}^t \sigma)
$$

$\sigma$ 和 $c_i$ 都是空间坐标的函数, 所以我们可以说它们构成了一个场, 这就是 Radiance Field (辐射场). 这样, 只要我们知道了某个场景的辐射场, 我们就能够从任何角度和方法, 以任何成像方法, 得到场景的二维图像.

在实际过程中, 无法直接进行积分, 于是我们在光路上采样 $\{t_i\}_{i=0}^N$ ($t_0=t_n, t_N = t_f$), 进行加权求和

$$
\tilde{C}(\mathbf{r}) = \sum_{i=1}^N (1 - exp(-\sigma_i\delta_i)) T_i c_i
$$

$\delta_i = t_i - t_{i-1}$; $\sigma_i, c_i$ 是采样点上的体密度值和颜色值; $T_i = exp(-\sum_{j=1}^{i-1} \sigma_i \delta_i)$ 是累积透明度. 

这个离散求和形式的体渲染公式看上去抽象, 所以我将尝试解释一下.

首先来看$(1 - exp(-\sigma_i \delta_i))$ , 这一项表征的是采样区间 $[t_{i-1}, t_i]$ 对于颜色的贡献程度, 也就是区间的不透明度. 在积分形式中, 这一项对应 $\sigma$ , 所以最直接的表达其实是 $\sigma_i \delta_i$ , 但考虑到后续我们要用利用神经网络进行优化, 为了保证数值稳定性, 我们需要对这一项进行了归一化, 保证区间的不透明度在 $[0, 1]$ 之间, 同时还要使最后的表达式是 $\sigma_i \delta_i$ 的增函数, 所以我们采用了这样的表达 (那为什么不用 $sigmoid$ 函数? 因为如果这样的话, $T_i$ 就很难给出合适的形式了).

接着就是 $T_i$ 这一项, 其对应于积分形式中的 $T$. 光路上的点对颜色的贡献($\sigma$)有大有小, 但从整体上来说, 离起点越远的点的贡献应该是更小的. 当一条光线从起点到终点, 每经过一个采样区间理应减弱一点, 变为以前 $\gamma$ 倍 ($\gamma < 1$), 这个倍数应该和该区间的不透明度(对颜色的贡献程度) $(1 - exp(-\sigma_i \delta_i))$ 负相关, 所以我们取 
$\gamma = 1 - (1 - exp(-\sigma\delta)) = exp(-\sigma \delta)$ . 直观上来说, 这个 $\gamma$ 其实就是某个区间的透明度! 这样 $T_i$ 其实就是之前所有区间的损失系数(也就是透明度)的累积.

有了离散的体渲染公式, 我们就知道如何从一个辐射场中进行渲染. 接下来我们主要问题就是: **如何构建辐射场?**

## Optional: 考虑吸收的体渲染模型

体渲染的积分形式也很抽象, 这个思想来自于考虑吸收的光学模型. 具体来说, 对于一条光路, 其上的每一个点都对于光线有一定的吸收效果, 吸收程度和光强成正比 (具体可见论文 [Optical Models for Direct Volume Rendering](https://courses.cs.duke.edu/spring03/cps296.8/papers/max95opticalModelsForDirectVolumeRendering.pdf)). 于是, 我们可以简单定义一个**吸收函数** 
$\tau(s)$ 

$$
\frac{dI}{ds} = -\tau(s)I
$$

其中 $I$ 是光强, $\tau(s)$ 是路径的函数. 

这是一个简单的常微分方程, 我们可以给出解

$$
I(s) = I_0 exp(-\int_0^s \tau(t)dt)
$$

$I_0$ 是入射光强, $[0, s]$ 范围内的透明度**为**

$$
T = exp(-\int_0^s \tau(t)dt)
$$

在上述过程中, 吸收函数描述的是光路上一点对于光强的吸收程度. 我们可以合理地推广, 认为吸收函数描述的是光路上每个点对光线的影响强度. 这样一来 NeRF 的公式就可以理解了.

## 用神经网络拟合辐射场

(怎么不算一种自监督)

构建辐射场, 其实就是得到 $F = (\sigma, c)$ 这个场函数. 我们可以发现 $F$ 实际上是空间坐标 $\mathbf{r}(t)$ 和方向 $\mathbf{d}$ 的函数, 也就是空间坐标 $\mathbf{x} \in \mathbb{R}^3$ 和观察角度 $\mathbf{d} \in \mathbb{R}^2$ 的函数 (回忆一下球坐标, 一个三维空间中方向实际上是二维的). 所以, 我们可以用一个神经网络来表征这个辐射场

$$
  F_\theta (\mathbf{x}, \mathbf{d}) = (\sigma, c) 
$$

接下来通过最小化体渲染图片和真实图片之间的 $L^2$ 误差, 我们就能够优化这个神经网络. 损失函数为

$$
L = \sum(||\tilde{C}(r) - C(r)||_2)
$$

其中 $\tilde{C}$ 为渲染颜色, $C$ 为真实颜色.

接下来我们具体来看看这个神经网络场的结构. 

![](/images/NeRF/image%201.png)

场的输入是位置 $\mathbf{x}$ 和方向 $\mathbf{d}$ , 输出的体密度 $\sigma$ 只依赖于位置, 而颜色 $c$ 同时依赖于位置和方向. 两个输入都会先被归一化到 $[0, 1]$ , 然后进过位置编码扩展为更高维的向量, 也就是对输入的每一个维度值 $p$ 进行编码

$$
\text{PosEncode}(p) = (sin(2^0p), cos(2^0 p), sin(2^1 p), cos(2^1 p), \dots , sin(2^{L-1} p), cos(2^{L-1} p))
$$

位置编码的动机相对单纯, 初始的输入只有五个维度, 且被归一化到了 $[0, 1]$ 中, 不同输入之间的距离非常小, 不利于模型学习, 所以利用这种方法进行升维, 使不同输入之间的距离增大.

在归一化和位置编码之后, 位置输入会经过一个 $\text{MLP}_1$ 得到体密度和位置特征 (体密度是), 然后位置特征会和方向输入一起经过另一个 $\text{MLP}_2$ 得到 RGB. 

$\text{MLP}_1$ 的输出包括两个部分, 第零维是该点的点密度, 其余的维度是该位置的特征, 这个特征和编码后的方向一起经过 $\text{MLP}_2$ 得到颜色特征. 事实上, 我们只需要修改一下 $\text{MLP}_2$ 的输出头, 我们也能让其输出法线方向, 贴图, 光照等等信息, 将颜色场扩展为其他渲染属性的场. 

在我的观点中, NeRF 方法其实是一种自监督学习方法. 自监督学习的一个较大应用是图像特征提取, AutoEncoder 是这个领域的经典框架, 它由两个神经网络 encoder 和 decoder 构成, encoder 将图像降为低维特征, decoder 将这个低维特征重新提升为原图像维度, 然后通过将最小化 decoder 的输出和输入图像的差距来联合优化 encoder 和 decoder. 这个思路其实是自监督学习的一般思路, NeRF 也基本符合. 但是一个比较大的区别是, 过去的方法主要希望得到泛化很强(比如扩展到训练集之外的图像特征提取)的网络, 而 NeRF 仅仅局限于同一个场景, 只是希望能够泛化到训练集中没有的视角上, 所以不同场景需要重新训练不同的 NeRF, 加上 NeRF 的训练其实较为缓慢, 这是 NeRF 的瓶颈和重要改进方向.

## 采样, regularization 以及其他